{"cells":[{"cell_type":"markdown","metadata":{"id":"EYYak4lmL7q8"},"source":["Note: Google collab doesn't support OpenCV. So please open it in Jupyter notebook or the local machine instead ðŸ˜€."]},{"cell_type":"markdown","metadata":{"id":"L7HpFx_v3GlK"},"source":["#Install modules\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQCu17SL1JbW"},"outputs":[],"source":["!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"mUCM7AiM0qSp"},"source":["#Import modules"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"pZvzH9gN4uO0"},"outputs":[],"source":["import cv2\n","import os\n","import numpy as np\n","import keras\n","import matplotlib.pyplot as plt\n","from tensorflow.keras.applications import VGG16\n","from keras import backend as K\n","from keras.models import Model, Sequential\n","from keras.layers import Input, Dense, Activation, LSTM\n","import h5py"]},{"cell_type":"markdown","metadata":{"id":"SGuHoYa91UrZ"},"source":["#Initial variables\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"_odMJZYI4uO3"},"outputs":[],"source":["DIR_TR = \"dataset/Train/\"\n","DIR_TE = \"dataset/Test/\"\n","DIR_VA = \"dataset/Valid/\"\n","\n","# Frame size  \n","IMG_SIZE = 224\n","IMG_SIZE_TUPLE = (IMG_SIZE, IMG_SIZE)\n","\n","# Number of channels (RGB)\n","NUM_CHANNELS = 3\n","# Flat frame size\n","IMG_SIZE_FLAT = IMG_SIZE**2 * NUM_CHANNELS\n","\n","# Number of classes for classification (Violence-No Violence)\n","NUM_CLASSES = 2\n","CLASSES_LIST = ['NonViolence', 'Violence']\n","assert len(CLASSES_LIST) == NUM_CLASSES\n","\n","SEQUENCE_LENGTH = 20"]},{"cell_type":"markdown","metadata":{"id":"IW52o0BB1yGt"},"source":["#VGG16 Model"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6081,"status":"ok","timestamp":1655104799552,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"nM5y_QMq4uO5","outputId":"158e9fca-a1e5-489c-d618-be6614c6828b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," fc1 (Dense)                 (None, 4096)              102764544 \n","                                                                 \n"," fc2 (Dense)                 (None, 4096)              16781312  \n","                                                                 \n"," predictions (Dense)         (None, 1000)              4097000   \n","                                                                 \n","=================================================================\n","Total params: 138,357,544\n","Trainable params: 138,357,544\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["image_model = VGG16(include_top=True, weights='imagenet')\n","image_model.summary()"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655104799552,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"z_roX6en4uO7","outputId":"a0123c32-f5a6-4beb-d56f-9b964736abfd"},"outputs":[],"source":["transfer_layer = image_model.get_layer('fc2')\n","image_model_transfer = Model(inputs=image_model.input,\n","                             outputs=transfer_layer.output)"]},{"cell_type":"markdown","metadata":{"id":"W6ESgRLM1igz"},"source":["#Frame extraction function"]},{"cell_type":"markdown","metadata":{"id":"C60BR9lT4xlz"},"source":["## Function"]},{"cell_type":"markdown","metadata":{"id":"rRyzInZ8Nv_0"},"source":["Get array of each frame"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"2fS-9egP4uO4"},"outputs":[],"source":["\n","def get_frames(vid_path):\n","    cap = cv2.VideoCapture(vid_path)\n","    \n","    new_seq = SEQUENCE_LENGTH\n","    \n","    total_frames = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n","    split_size = int(min(3, max(total_frames // new_seq, 1)))\n","    chunk_size = total_frames / split_size\n","    skip_frame = max(1, chunk_size // new_seq)\n","    \n","    for c in range(split_size):\n","        initial_frame = new_seq * c * skip_frame\n","        \n","        frames_list = []\n","        for i in range(new_seq):\n","            cap.set(cv2.CAP_PROP_POS_FRAMES, (i * skip_frame) + initial_frame)\n","            ret, frame = cap.read()\n","            if not ret:\n","                break\n","            \n","            RGB_img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                \n","            res = cv2.resize(RGB_img, \n","                             dsize=IMG_SIZE_TUPLE,\n","                             interpolation=cv2.INTER_CUBIC)\n","            \n","            frames_list.append(res)\n","\n","            \n","        if len(frames_list) != SEQUENCE_LENGTH:\n","            continue\n","        \n","        result = np.array(frames_list)\n","        result = (result / 255).astype(np.float16)\n","        \n","        yield result"]},{"cell_type":"markdown","metadata":{"id":"4rmzysvqNxLb"},"source":["Extract features each frame from video"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"VuIv-yZ64uO8"},"outputs":[],"source":["def extracted_features(vid_path):\n","    gen = get_frames(vid_path)\n","    for chunk in gen:\n","        shape = (SEQUENCE_LENGTH, 4096)\n","        transfer_values = np.zeros(shape=shape, dtype=np.float16)\n","        transfer_values = image_model_transfer.predict(chunk)\n","        \n","        yield transfer_values\n","    "]},{"cell_type":"markdown","metadata":{"id":"63H6kq4JN6bt"},"source":["Create dataset"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"dTS1Qr3R4uO9"},"outputs":[],"source":["def create_dataset(dataset_dir):\n","    class_dataset_dir = os.listdir(dataset_dir)\n","    features = []\n","    labels = []\n","    \n","    \n","    for dir in class_dataset_dir:\n","        videos_list = os.listdir(os.path.join(dataset_dir, dir))\n","        count = 0\n","        for vid in videos_list:\n","            gen_path = os.path.join(dataset_dir, dir, vid)\n","            gen = extracted_features(gen_path)\n","            for chunk in gen:\n","                features.append(chunk)\n","                if dir == CLASSES_LIST[0]:\n","                    labels.append([1, 0])\n","                elif dir == CLASSES_LIST[1]:\n","                    labels.append([0, 1])\n","            count += 1\n","            \n","            print(dir, '{:.2f}%'.format((count /len(videos_list))*100))\n","    return features, labels"]},{"cell_type":"markdown","metadata":{"id":"uuhfr6dS163I"},"source":["##Create dataset for video"]},{"cell_type":"markdown","metadata":{"id":"okGYhtx940P2"},"source":["Note: Please skip this section if you already save the dataset."]},{"cell_type":"code","execution_count":17,"metadata":{"id":"bqBqBvp24uO9","outputId":"ce8df721-b6c5-4dcb-8ad3-7a2ee92bbbe3"},"outputs":[{"ename":"FileNotFoundError","evalue":"[WinError 3] The system cannot find the path specified: 'dataset/Train/'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32mc:\\Users\\monChinawat\\Documents\\GitHub\\ViolenceDetection\\notebook.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000020?line=0'>1</a>\u001b[0m features_tr, labels_tr \u001b[39m=\u001b[39m create_dataset(DIR_TR)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000020?line=1'>2</a>\u001b[0m features_te, labels_te \u001b[39m=\u001b[39m create_dataset(DIR_TE)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000020?line=2'>3</a>\u001b[0m features_va, labels_va \u001b[39m=\u001b[39m create_dataset(DIR_VA)\n","\u001b[1;32mc:\\Users\\monChinawat\\Documents\\GitHub\\ViolenceDetection\\notebook.ipynb Cell 18'\u001b[0m in \u001b[0;36mcreate_dataset\u001b[1;34m(dataset_dir)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000017?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_dataset\u001b[39m(dataset_dir):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000017?line=1'>2</a>\u001b[0m     class_dataset_dir \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(dataset_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000017?line=2'>3</a>\u001b[0m     features \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/monChinawat/Documents/GitHub/ViolenceDetection/notebook.ipynb#ch0000017?line=3'>4</a>\u001b[0m     labels \u001b[39m=\u001b[39m []\n","\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'dataset/Train/'"]}],"source":["features_tr, labels_tr = create_dataset(DIR_TR)\n","features_te, labels_te = create_dataset(DIR_TE)\n","features_va, labels_va = create_dataset(DIR_VA)"]},{"cell_type":"markdown","metadata":{"id":"0a9xD8Uo2Kk2"},"source":["##Write and load dataset"]},{"cell_type":"markdown","metadata":{"id":"7O7b43eD5CFQ"},"source":["After you finish the create dataset section. Uncomment this below code to save the dataset and when you've done, Comment it again to make sure you won't overwrite it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-u_VylM4uO_"},"outputs":[],"source":["# with h5py.File('features_labels/datav6.h5', mode='w') as file:\n","#     file.create_dataset('X_tr', data=features_tr)\n","#     file.create_dataset('X_te', data=features_te)\n","#     file.create_dataset('X_va', data=features_va)\n","    \n","#     file.create_dataset('y_tr', data=labels_tr)\n","#     file.create_dataset('y_te', data=labels_te)\n","#     file.create_dataset('y_va', data=labels_va)"]},{"cell_type":"markdown","metadata":{"id":"-FIdiHdB5wm2"},"source":["Load the dataset from hdf5 file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfH-6WIJ4uPA"},"outputs":[],"source":["with h5py.File('features_labels/datav6.h5', mode='r') as file:\n","    X_tr = file['X_tr'][:]\n","    X_te = file['X_te'][:]\n","    X_va = file['X_va'][:]\n","    \n","    y_tr = file['y_tr'][:]\n","    y_te = file['y_te'][:]\n","    y_va = file['y_va'][:]"]},{"cell_type":"markdown","metadata":{"id":"hipjZvk-2ZAC"},"source":["##Check data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":17,"status":"ok","timestamp":1655104818688,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"IOhAJ4N45zVs","outputId":"983c332a-47cb-416c-bdf6-27a4a7defe19"},"outputs":[],"source":["print(\"Train features\", len(X_tr))\n","print(\"Test features\", len(X_te))\n","print(\"Validation features\", len(X_va))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"414FeY_K54Gy"},"outputs":[],"source":["print(\"Train label\", len(y_tr))\n","print(\"Test label\", len(y_te))\n","print(\"Validation label\", len(y_va))"]},{"cell_type":"markdown","metadata":{"id":"cWywynk42gMd"},"source":["#LSTM model"]},{"cell_type":"markdown","metadata":{"id":"uk2CPIey2uxw"},"source":["##Create model\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Np_Fohrf4uPA"},"outputs":[],"source":["chunk_size = 4096\n","n_chunks = SEQUENCE_LENGTH\n","rnn_size = 512\n","\n","model = Sequential()\n","model.add(LSTM(rnn_size, input_shape=(n_chunks, chunk_size)))\n","model.add(Dense(1024))\n","model.add(Activation('relu'))\n","model.add(Dense(50))\n","model.add(Activation('sigmoid'))\n","model.add(Dense(2))\n","model.add(Activation('softmax'))\n"]},{"cell_type":"markdown","metadata":{"id":"rjw0Y4Hc2zPb"},"source":["##Train model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":171588,"status":"ok","timestamp":1655105682808,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"RwmKCz_C4uPB","outputId":"df0155d1-0946-4bd3-9e24-792ac3c76448"},"outputs":[],"source":["from keras.callbacks import EarlyStopping\n","from tensorflow.keras.optimizers import Adam\n","epoch = 200\n","batchS = 500\n","\n","early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience = 15, mode = 'min', restore_best_weights = True)\n","\n","opt = Adam(learning_rate=0.001)\n","model.compile(loss='mean_squared_error', optimizer=opt, metrics=['accuracy'])\n","history = model.fit(x=np.array(X_tr), y=np.array(y_tr), epochs=epoch,\n","                    validation_data=(np.array(X_va), np.array(y_va)),\n","                    shuffle=True,\n","                    batch_size=batchS, verbose=2,\n","                    callbacks=[early_stopping_callback])"]},{"cell_type":"markdown","metadata":{"id":"vIEqtPFt245s"},"source":["##Evaluate model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21748,"status":"ok","timestamp":1655105717939,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"mD99rIja4uPB","outputId":"38c637be-04b4-49fa-d7f1-d2d96ed9e9f9"},"outputs":[],"source":["result = model.evaluate(np.array(X_te), np.array(y_te))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":424,"status":"ok","timestamp":1655104549612,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"AYRx_fEa4uPB","outputId":"653ee12e-c669-4092-bb94-7e14d18ada6d"},"outputs":[],"source":["for name, value in zip(model.metrics_names, result):\n","    print(name, value)"]},{"cell_type":"markdown","metadata":{"id":"ZR8mWhJB29Kk"},"source":["##Plot graph."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":646},"executionInfo":{"elapsed":1029,"status":"ok","timestamp":1655105735924,"user":{"displayName":"à¸Šà¸´à¸™à¸§à¸±à¸•à¸£ à¸™à¸²à¹„à¸Šà¸¢à¸˜à¸‡","userId":"10452193718542966036"},"user_tz":-420},"id":"IcxmQpQq4uPC","outputId":"4414cc9b-ca33-49f9-df9d-339b6b8d14d8"},"outputs":[],"source":["plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('model accuracy')\n","plt.ylabel('accuracy')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path.eps', format='eps', dpi=1000)\n","plt.show()\n","# summarize history for loss\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.savefig('destination_path1.eps', format='eps', dpi=1000)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"mVopoxc73AM2"},"source":["##Save model and save weights."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BFQSVBnN9tje"},"outputs":[],"source":["model.save('model/vggLSTM/modelv4_2.h5')\n","model.save_weights('model/vggLSTM/model_weightsv4_2.h5')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["C60BR9lT4xlz","uk2CPIey2uxw","rjw0Y4Hc2zPb","ZR8mWhJB29Kk","mVopoxc73AM2"],"name":"Violence Detection.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.10.4 ('violencedetection')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"8e7fdd385a006bab1d54cce523f040101c70cd6998bbc817cbd96698aaa7d552"}}},"nbformat":4,"nbformat_minor":0}
